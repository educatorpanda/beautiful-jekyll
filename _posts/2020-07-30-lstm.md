---
layout: post
title: Everything you need to know about LSTM!
subtitle: Machine Learning Episode-1.X
gh-repo: educatorpanda/educatorpanda.github.io
gh-badge: [star, follow]
comments: true
readtime: true
tags: [Machine Learning, Python, Neural Networks]
comments: true
---

### Introduction

As the title goes, in this post I will try to explain everthing you need to know about LSTM in a concise, simple and illustrative manner avoiding all the jargons that will try to obstruct our learning! Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of [RNN](https://educatorpanda.github.io/2020-07-28-linear-regression/), which work tremendously well on a large variety of problems, and are now widely used. Before you go through this post, I would highly recommend you to go through [this](https://educatorpanda.github.io/2020-07-28-linear-regression/) post that explains everthing you need to know about RNN which will help you understand the content of the following article clearly.

### Recap of RNN network

Suppose we are trying to predict the last word in:

> “I live in France and I can speak ___________ fluently” 

We don’t need any further context – it’s pretty obvious the next word is going to be ***"French"***. In such cases, where we are dealing with short-term dependencies, RNNs can learn to use the past information. We have already established in our post regarding RNN that they are great when it comes to short contexts, but in order to be able to build a story and remember it, like:

> “My name is Educator Panda and I had spent 25 years of my life working in India. I currently live in France and my native language is ____________ ”

we need more context. We need our model to be able to understand and remember the context, just like a human brain. This is not possible with a simple RNN. RNN remembers things for just small durations of time, i.e. if we need the information after a small time it may be reproducible, but once a lot of words are fed in, this information gets lost somewhere. This issue can be resolved by applying a slightly modified version of RNNs – the Long Short-Term Memory Networks.

### LSTM architechture

In order to get the intuitive understanding of the network architechture of LSTM, we will consider the following example. Assume you are a journalist of a newspaper and you are covering a bank robbery case in which you have written the following article in your newspaper:

> A gang named **X1** was found stealing the money from the cashiers department in **Y1** bank (statement C1)

So, wouldn't this be the headline in your newspaper? **Y1 bank robbed by gang X1** (Headline H1)

Now, after further investigations you found out that the gang who was stealing the money from the cashiers department in **Y1** bank was not **X1**, but **Z1** (input I2). So, the article in your newspaper will be modified as follows:

> A gang named **Z1** was found stealing the money from the cashiers department in **Y1** Bank (statement C2)

The headline would change to **Y1 bank robbed by gang Z1** (Headline H2)

As soon as you received the new information, you notice one thing that only the cell state and the hidden state of your memory cell has modified (Oh! I am sorry, I was not supposed to use the jargons). What I meant is that only the ***article*** (the one in the block quote) and the ***headline*** have changed in your newspaper (from C1 to C2 and H1 to H2 respectively).

So, if you consider it this way, then the term **Memory blocks or LSTM cells** is equivalent to the **newspaper** where all the processing and modification of the article and headline takes place. The term **Cell state** and **Hidden state** of an LSTM cell are analogous to the ***article*** (which tells us about the current piece of information we have in the Newspaper) and the Newspaper ***headline*** respectively. The rectangles that we see in the image is the newspaper (or the LSTM cell). 


The manipulations to this cell is done through three major mechanisms, called gates. Each of them is being discussed in detail below.

### Forget Gate

As soon as you recieve the new input I2, the first thing that you would want to do is to **forget** or remove the redundant information from the cell state C1, i.e. throw away the name of the gang **X1** from C1. This process of forgetting **X1** is brought about by the forget gate. This gate takes in two inputs; **h<sub>t-1</sub>** and **x<sub>t</sub>**. **h<sub>t-1</sub>** is the hidden state from the previous cell or the output of the previous cell (for our case it is the heading H1) and **x<sub>t</sub>** is the input at that particular time step (for our case it is I2). Also let us call the previous cell state C1 as **C<sub>t-1</sub>**. Since vectors are the natural language of neural networks, mathematically, **C<sub>t-1</sub>**, **h<sub>t-1</sub>** and **x<sub>t</sub>** are just vectors of some arbitrarily chosen length. 

Let us understand how to forget redundant information mathematically. The given hidden state (**h<sub>t-1</sub>**) and input (**x<sub>t</sub>**) are multiplied by the weight matrices and a bias is added. We then squeeze the output between 0 and 1 using sigmoid activation function which is responsible for deciding which values to keep and which to discard. If we get ‘0’ as output, it means that the forget gate wants the cell state to forget that piece of information completely (i.e., throw **X1** from C1). Similarly, a ‘1’ means that the forget gate wants to remember that entire piece of information (keep **X1** in C1). Let us call this output as **f<sub>t</sub>**. You can also think of this 
**f<sub>t</sub>** as a switch which is closed if it is '1' i.e., it will allow the electrons (information) to flow through the wire (cell state) and open if it is '0' i.e., it will not allow the electrons (information) to flow through the wire (cell state)

When we multiply this output **f<sub>t</sub>** with **C<sub>t-1</sub>**, we ensure that the redundant information from C1 is removed. Let us call this output as **fC<sub>t</sub>**.

### Input Gate 

The next step is to add the useful information from I2 to the cell state C1, i.e., the gang **Z1** is added to C1. This process of adding **Z1** is brought about by the input gate. This gate again takes in two inputs; **h<sub>t-1</sub>** and **x<sub>t</sub>**. This addition of information is basically three-step process. 

1. A sigmoid layer decides which values we’ll update. This is very similar to the forget gate i.e., **h<sub>t-1</sub>** and **x<sub>t</sub>** are multiplied by the weight matrices and a bias is added (weight matrices and bias are different from the weight matrices and bias of the forget gate) and this output is squeezed between 0 and 1. Let us call this output as **i<sub>t</sub>**.
2. Next we create a vector **g<sub>t</sub>** containing all possible values i.e. combination of H1 and I2 (both useful and redundant), so that after we multiply this vector to **i<sub>t</sub>** (which is the 3rd step), we get only the useful information that can be added to the cell state C1. So, we pass the hidden state **h<sub>t-1</sub>** and current input **x<sub>t</sub>** into the tanh function (another activation function) to squish values between -1 and 1 to help regulate the network and obtain the value **g<sub>t</sub>**.
3. Lastly, as mentioned, we just multiply **g<sub>t</sub>** and **i<sub>t</sub>** to get only the useful information that can be added to the cell state C1. Let us call this output as **gi<sub>t</sub>**  

Now, just add **gi<sub>t</sub>** to **fC<sub>t</sub>** to get **C<sub>t</sub>** which will be our new cell state C2. Let us call this C2 as **C<sub>t</sub>**

### Output Gate

Till now, we have removed the redundant information from C1 and added new useful information to C1 using forget gate and input gate respectively such that our new cell state C2 (**C<sub>t</sub>**) is formed! Finally, we need the new headline H2 for article C2. This will be done using output gate. As we know that the Headline of an article is just the short (one sentence) summary of the article, we will again create a sigmoidal switch that will filter the article C2 by allowing only the most important information to pass through it (it will decide what parts of the cell state we’re going to output). 

This is very similar to the forget and input gate i.e., **h<sub>t-1</sub>** and **x<sub>t</sub>** are multiplied by the weight matrices and a bias is added (weight matrices and bias are different from the weight matrices and bias of the forget and input gate) and this output is squeezed between 0 and 1. Let us call this output as **o<sub>t</sub>**. Then, we put the cell state through tanh (to squeeze the values between −1 and 1) and multiply it by **o<sub>t</sub>**, so that we only output the parts we decided to. This way we will get our new Headline H2 (our new hidden state)! Let us call it as **h<sub>t</sub>**.

When we write down the equations of LSTMs, they look pretty intimidating. Hopefully, this explaination made the understanding of LSTMs less painful and more pleasing!

### Practical demonstration of LSTM using Keras in Python

What we will be building today is **Character-level Neural Language Model in Keras**. A language model predicts the next word/character sequence based on the specific words/characters that have come before it in the sequence.

**Importing Dependencies**

Let us first import all the important dependencies.

{% highlight python linenos %}
from numpy import array
from keras.utils import to_categorical
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
{% endhighlight %}

**Load Data**

Here we will use the first sentence of the famous Aesop fables "**The Fox & the Crow**" as our dataset. Let us have a look at the data:

> One bright morning as the Fox was following his sharp nose through the wood in search of a bite to eat, he saw a Crow on the limb of a tree overhead. 

Let us store it in a variable ***raw_text***

{% highlight python linenos %}
raw_text = """One bright morning as the Fox was following his sharp nose through the wood in search of a bite to eat, he saw a Crow on the limb of a tree overhead."""
{% endhighlight %}

**Preprocess**

We now need to define the training data for the network. There is a lot of flexibility in how you choose to break up the text and expose it to the network during training.

In this tutorial we will create a function ***preprocess*** which will split the string ***raw_text*** in chunk of fixed length of 15 characters. For example, your sequences should look like ***sequences = ['One bright morni', 'ne bright mornin', 'e bright morning', ...]***

{% highlight python linenos %}
def preprocess(raw_text,length):
    sequences = list()
    for i in range(length, len(raw_text)):
      seq = raw_text[i-length:i+1]
      sequences.append(seq)
    return sequences
 
sequences = preprocess(raw_text,length = 15)
{% endhighlight %}

let us print the length of the sequences

{% highlight python linenos %}
print(len(sequences))
{% endhighlight %}

{% highlight python linenos %}
134
{% endhighlight %}

**Encode Sequences**

We cannot model the characters directly, instead we must map the characters to integers. We can do this easily by creating a function ***mapp*** which will first create a set of all of the distinct characters in the ***raw_text***, then create a map of each character to a unique integer.

{% highlight python linenos %}
def mapp(raw_text):
    chars = sorted(list(set(raw_text)))
    mapping = dict((c, i) for i, c in enumerate(chars))
    return mapping
    
mapping = mapp(raw_text)
vocab_size = len(mapping)
{% endhighlight %}

For example, the list of unique sorted characters in the ***raw_text*** is as follows:

{% highlight python linenos %}
[' ', ',', '.', 'C', 'F', 'O', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x']
{% endhighlight %}

Let us also have a look at mapping and the size of this vocabulary.

{% highlight python linenos %}
27
{' ': 0, ',': 1, '.': 2, 'C': 3, 'F': 4, 'O': 5, 'a': 6, 'b': 7, 'c': 8, 'd': 9, 'e': 10, 'f': 11, 'g': 12, 'h': 13, 'i': 14, 'l': 15, 'm': 16, 'n': 17, 'o': 18, 'p': 19, 'r': 20, 's': 21, 't': 22, 'u': 23, 'v': 24, 'w': 25, 'x': 26}
{% endhighlight %}

Next, we will create a function ***encode*** which will encode all the sequences to corroponding dictionary 'mapping' and store it in the 'sequences_encode' 2D array

For E.g: sequences[0] = 'One bright morni'
sequences_encode[0] = [5, 17, 10, 0, 7, 20, 14, 12, 13, 22, 0, 16, 18, 20, 17, 14]
where 'O'=5, 'n'=17, 'e'=10, etc. is found dictionary 'mapping'

{% highlight python linenos %}
def encode(sequences,mapping):
    sequences_encode = list()
    for line in sequences:
      encoded_seq = [mapping[char] for char in line]
      sequences_encode.append(encoded_seq)
    sequences_encode = array(sequences_encode)
    return sequences_encode
    
sequences_encode = encode(sequences,mapping)
{% endhighlight %}

Let us have a look at first 5 entries of 'sequences_encode' 2D array

{% highlight python linenos %}
print(sequences_encode[:5])
{% endhighlight %}

{% highlight python linenos %}
array([[ 5, 17, 10,  0,  7, 20, 14, 12, 13, 22,  0, 16, 18, 20, 17, 14],
       [17, 10,  0,  7, 20, 14, 12, 13, 22,  0, 16, 18, 20, 17, 14, 17],
       [10,  0,  7, 20, 14, 12, 13, 22,  0, 16, 18, 20, 17, 14, 17, 12],
       [ 0,  7, 20, 14, 12, 13, 22,  0, 16, 18, 20, 17, 14, 17, 12,  0],
       [ 7, 20, 14, 12, 13, 22,  0, 16, 18, 20, 17, 14, 17, 12,  0,  6]])
{% endhighlight %}

### Split Input and Output

Now that the sequences have been integer encoded, we will define ***split*** function that will separate the columns into input and output sequences of characters.

{: .box-note}
**Note:** First 14 characters will be our input and 15h character will be our output corrosponding to it.

{% highlight python linenos %}
def split(sequences_encode):
    X, y = sequences_encode[:,:-1], sequences_encode[:,-1]
    return X, y
    
X, y = split(sequences_encode)
{% endhighlight %}

Let us have a look at the first sample

{% highlight python linenos %}
print(X[0])
print(y[0])
{% endhighlight %}

{% highlight python linenos %}
[5 17 10 0 7 20 14 12 13 22 0 16 18 20 17] 
14
{% endhighlight %}

Now that we have prepared our training data we need to transform it so that it is suitable for use with Keras.

First we must transform the list of input and output sequences into the form **[samples, time steps, features]** and **[samples, features]** expected by an LSTM network. For that we will convert them into a one hot encoding. Each **X** and **y** value is converted into a sparse vector with a length of vocab_size, full of zeros except with a 1 in the column for the charater (integer) that the pattern represents.

{% highlight python linenos %}
X = array([to_categorical(x, num_classes=vocab_size) for x in X])
y = to_categorical(y, num_classes=vocab_size)
{% endhighlight %}

---
layout: post
title: Everything you need to know about LSTM!
subtitle: Machine Learning Episode-1.X
gh-repo: educatorpanda/educatorpanda.github.io
gh-badge: [star, follow]
comments: true
readtime: true
tags: [Machine Learning, Python, Neural Networks]
comments: true
---

### Introduction

As the title goes, in this post I will try to explain everthing you need to know about LSTM in a concise, simple and illustrative manner avoiding all the jargons that will try to obstruct our learning! Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of [RNN](https://educatorpanda.github.io/2020-07-28-linear-regression/), which work tremendously well on a large variety of problems, and are now widely used. Before you go through this post, I would highly recommend you to go through [this](https://educatorpanda.github.io/2020-07-28-linear-regression/) post that explains everthing you need to know about RNN which is a sort of pre-requisite to help you understand the content of the following article clearly.

### Recap of RNN network

Suppose we are trying to predict the last word in:

> “I live in France and I can speak _____________ fluently” 

We don’t need any further context – it’s pretty obvious the next word is going to be ***"French"***. In such cases, where we are dealing with short-term dependencies, RNNs can learn to use the past information. We have already established in our post regarding RNN that they are great when it comes to short contexts, but in order to be able to build a story and remember it, like:

> “My name is Educator Panda and I had spent 25 years of my life working in India. I currently live in France and my native language is ____________”

we need more context. We need our model to be able to understand and remember the context, just like a human brain. This is not possible with a simple RNN. RNN remembers things for just small durations of time, i.e. if we need the information after a small time it may be reproducible, but once a lot of words are fed in, this information gets lost somewhere. This issue can be resolved by applying a slightly modified version of RNNs – the Long Short-Term Memory Networks.

### LSTM architechture

In order to get the intuitive understanding of the network architechture of LSTM, we will consider the following example. Assume you are a journalist of a newspaper and you are covering a bank robbery case in which you found out the following information:

> The security guard of the bank was found helping the burglars in stealing the money from the cashiers department

Now, after further investigations you found out this new and final piece of information

> The **manager** of the bank **ordered** the **security guard** to **help the burglars**.

So, in order to register this new piece of information into your brain, your will certainly perform following 3 operations:

1. You definitely don't want to remember each and every word from the above information. You would most likely be remembering the words most important to you (or the text in bold). For this you would want to **forget** the redundant information.
2. 



LSTM cells instead take information and make minor modifications (like additions or multiplications) to it while it flows through.

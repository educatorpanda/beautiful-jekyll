---
layout: post
title: Everything you need to know about LSTM!
subtitle: Machine Learning Episode-1.X
gh-repo: educatorpanda/educatorpanda.github.io
gh-badge: [star, follow]
comments: true
readtime: true
tags: [Machine Learning, Python, Neural Networks]
comments: true
---

### Introduction

As the title goes, in this post I will try to explain everthing you need to know about LSTM in a concise, simple and illustrative manner avoiding all the jargons that will try to obstruct our learning! Long Short Term Memory networks – usually just called “LSTMs” – are a special kind of [RNN](https://educatorpanda.github.io/2020-07-28-linear-regression/), which work tremendously well on a large variety of problems, and are now widely used. Before you go through this post, I would highly recommend you to go through [this](https://educatorpanda.github.io/2020-07-28-linear-regression/) post that explains everthing you need to know about RNN which is a sort of pre-requisite to help you understand the content of the following article clearly.

### Recap of RNN network

Suppose we are trying to predict the last word in:

> “I live in France and I can speak ...... fluently” 

We don’t need any further context – it’s pretty obvious the next word is going to be ***"French"***. In such cases, where we are dealing with short-term dependencies, RNNs can learn to use the past information. We have already established in our post regarding RNN that they are great when it comes to short contexts, but in order to be able to build a story and remember it, like:

> “My name is Educator Panda and I had spent 25 years of my life working India. I currently live in France and my native language is ......”

we need more context. We need our model to be able to understand and remember the context, just like a human brain. This is not possible with a simple RNN. 
